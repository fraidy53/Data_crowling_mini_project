"""
í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
ì—¬ëŸ¬ ì§€ì—­ í¬ë¡¤ëŸ¬ë¥¼ í†µí•© ê´€ë¦¬
"""

import pandas as pd
from typing import List, Dict
import logging

# ì§€ì—­ë³„ í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
from regional.seoul.seoul_shinmun import SeoulShinmunCrawler
from regional.gyeonggi.gyeonggi_ilbo import GyeonggiIlboCrawler
from regional.gangwon.gangwon_domin_ilbo import GangwonDominIlboCrawler
from regional.chungcheong.daejon_ilbo import ChungcheongCrawler
from regional.gyeongsang.busan_ilbo import GyeongsangCrawler
from regional.jeolla.jeonnam_ilbo import JeollaCrawler

# ë°ì´í„°ë² ì´ìŠ¤ ë° í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
from database_manager import DatabaseManager
from text_file_saver import TextFileSaver

logger = logging.getLogger('CrawlerManager')


class CrawlerManager:
    """ì§€ì—­ë³„ í¬ë¡¤ëŸ¬ë¥¼ í†µí•© ê´€ë¦¬"""

    def __init__(self, use_database: bool = True, save_text_files: bool = True):
        """
        Args:
            use_database: ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            save_text_files: í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì—¬ë¶€
        """
        self.crawlers = []
        self.all_articles = []
        self.region_stats = {}

        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
        self.use_database = use_database
        if use_database:
            self.db_manager = DatabaseManager()

        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
        self.save_text_files = save_text_files
        if save_text_files:
            self.text_saver = TextFileSaver()

    def register_crawler(self, crawler):
        """í¬ë¡¤ëŸ¬ ë“±ë¡"""
        self.crawlers.append(crawler)
        logger.info(f"âœ“ {crawler.newspaper_name} í¬ë¡¤ëŸ¬ ë“±ë¡")

    def register_all_crawlers(self):
        """ëª¨ë“  ì§€ì—­ í¬ë¡¤ëŸ¬ ê¸°ë³¸ ë“±ë¡"""
        crawlers_list = [
            SeoulShinmunCrawler(),
            GyeonggiIlboCrawler(),
            GangwonDominIlboCrawler(),
            ChungcheongCrawler(),
            GyeongsangCrawler(),
            JeollaCrawler(),
        ]

        for crawler in crawlers_list:
            self.register_crawler(crawler)

    def run_by_region(self, region: str, max_articles: int = 50) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì˜ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰

        Args:
            region: ì§€ì—­ëª… (ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°ë„', 'ê°•ì›ë„')
            max_articles: ì‹ ë¬¸ì‚¬ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        """
        target_crawlers = [c for c in self.crawlers if c.region == region]

        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ•·ï¸  [{region}] í¬ë¡¤ë§ ì‹œì‘ ({len(target_crawlers)}ê°œ ì‹ ë¬¸)")
        logger.info(f"{'=' * 60}\n")

        for crawler in target_crawlers:
            articles = crawler.crawl(max_articles=max_articles)
            self.all_articles.extend(articles)

        return self.all_articles

    def run_all_crawlers(self, max_articles: int = 50) -> List[Dict]:
        """
        ëª¨ë“  ì§€ì—­ì˜ ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰

        Args:
            max_articles: ì‹ ë¬¸ì‚¬ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        """
        logger.info(f"\n\n{'=' * 70}")
        logger.info("ğŸ•·ï¸  [ì „ì²´] ì§€ì—­ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"    - í¬ë¡¤ëŸ¬ ìˆ˜: {len(self.crawlers)}ê°œ")
        logger.info(f"    - ì‹ ë¬¸ì‚¬ë‹¹ ê¸°ì‚¬ ìˆ˜: {max_articles}ê°œ")
        logger.info(f"{'=' * 70}\n")

        for idx, crawler in enumerate(self.crawlers, 1):
            logger.info(f"[{idx}/{len(self.crawlers)}] {crawler.newspaper_name}({crawler.region})")
            articles = crawler.crawl(max_articles=max_articles)
            self.all_articles.extend(articles)

            # ì§€ì—­ë³„ í†µê³„
            self.region_stats[crawler.region] = self.region_stats.get(crawler.region, 0) + len(articles)

        logger.info(f"\n{'=' * 70}")
        logger.info(f"âœ“ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        logger.info(f"{'=' * 70}\n")

        return self.all_articles

    def to_dataframe(self) -> pd.DataFrame:
        """ëª¨ë“  ê¸°ì‚¬ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        if not self.all_articles:
            return pd.DataFrame()
        return pd.DataFrame(self.all_articles).sort_values('date', ascending=False).reset_index(drop=True)

    def save_to_csv(self, filename: str = '../data/regional_news.csv'):
        """CSV íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€/ì—…ë°ì´íŠ¸)"""
        df = self.to_dataframe()
        if df.empty:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        import os
        import tempfile
        import shutil
        import gc
        
        # ì ˆëŒ€ ê²½ë¡œ ê³„ì‚° (í¬ë¡¤ëŸ¬ëŠ” src/crawlers/ì— ìˆìŒ)
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        csv_path = os.path.join(project_root, 'data', 'regional_news.csv')
        
        # ê¸°ì¡´ CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ì½ê¸°
        if os.path.exists(csv_path):
            try:
                existing_df = pd.read_csv(csv_path, low_memory=False)
                logger.info(f"ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ: {len(existing_df)}ê°œ ê¸°ì‚¬")
                
                # ìƒˆë¡œìš´ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„° í•©ì¹˜ê¸°
                combined_df = pd.concat([df, existing_df], ignore_index=True)
                
                # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ìµœì‹  ë°ì´í„° ìš°ì„ )
                combined_df = combined_df.drop_duplicates(subset=['url'], keep='first')
                
                # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                combined_df = combined_df.sort_values('date', ascending=False).reset_index(drop=True)
                
                logger.info(f"ê¸°ì¡´ + ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•©: {len(combined_df)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ ì œê±°)")
                df = combined_df
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del existing_df
                gc.collect()
                
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}. ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        
        # CSV ì €ì¥
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        temp_csv = csv_path + '.tmp'
        
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì €ì¥
                df.to_csv(temp_csv, index=False, encoding='utf-8-sig')
                gc.collect()
                
                # ê¸°ì¡´ íŒŒì¼ ì œê±° ì‹œë„
                if os.path.exists(csv_path):
                    try:
                        os.remove(csv_path)
                    except OSError as e:
                        logger.debug(f"ê¸°ì¡´ íŒŒì¼ ì œê±° ì‹¤íŒ¨ (ì¬ì‹œë„): {e}")
                        retry_count += 1
                        if retry_count < max_retries:
                            import time
                            time.sleep(0.5)
                            continue
                        else:
                            # íŒŒì¼ì„ ëª» ì œê±°í•´ë„ ì›ë³¸ì„ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì €ì¥í•˜ê³  ìƒˆ ë‚´ìš©ìœ¼ë¡œ ë®ì–´ì“°ê¸°
                            backup = csv_path + '.bak'
                            if os.path.exists(backup):
                                try:
                                    os.remove(backup)
                                except:
                                    pass
                            shutil.copy(csv_path, backup)
                            logger.debug(f"ê¸°ì¡´ íŒŒì¼ì„ {backup}ìœ¼ë¡œ ë°±ì—…")
                
                # ì„ì‹œ íŒŒì¼ì„ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
                if os.path.exists(temp_csv):
                    shutil.move(temp_csv, csv_path)
                    
                logger.info(f"\nâœ“ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
                logger.info(f"  - ì „ì²´ ê¸°ì‚¬: {len(df)}ê°œ")
                logger.info(f"  - ìˆ˜ì§‘ ì§€ì—­: {sorted(df['region'].unique().tolist())}")
                logger.info(f"  - ìˆ˜ì§‘ ì‹ ë¬¸: {sorted(df['source'].unique().tolist())}")
                break
                
            except Exception as e:
                logger.debug(f"CSV ì €ì¥ ì‹œë„ #{retry_count + 1} ì‹¤íŒ¨: {e}")
                retry_count += 1
                if retry_count >= max_retries:
                    # ë§ˆì§€ë§‰ ì‹œë„ - ì„ì‹œ íŒŒì¼ ê²½ë¡œì— ì €ì¥
                    fallback_path = csv_path + '_latest.csv'
                    try:
                        df.to_csv(fallback_path, index=False, encoding='utf-8-sig')
                        logger.error(f"CSV íŒŒì¼ì„ {fallback_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
                        logger.error(f"ì›ë³¸ íŒŒì¼ ({csv_path})ì´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤")
                        raise PermissionError(f"íŒŒì¼ì´ ì ê¸ˆ ìƒíƒœì…ë‹ˆë‹¤. ëŒ€ì‹  {fallback_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.") from e
                    except Exception as fallback_error:
                        logger.error(f"ëª¨ë“  ì €ì¥ ì‹œë„ ì‹¤íŒ¨: {fallback_error}")
                        raise
                else:
                    import time
                    time.sleep(0.5)

    def save_to_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not self.use_database:
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not self.all_articles:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"\n{'=' * 70}")
        logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        logger.info(f"{'=' * 70}")

        # ê¸°ì‚¬ ì €ì¥
        inserted = self.db_manager.insert_articles(self.all_articles)

        # ì§€ì—­ë³„ í†µê³„ ì—…ë°ì´íŠ¸
        for region, count in self.region_stats.items():
            newspapers = [a['newspaper'] for a in self.all_articles if a['region'] == region]
            for newspaper in set(newspapers):
                news_count = sum(1 for a in self.all_articles if a['region'] == region and a['newspaper'] == newspaper)
                self.db_manager.update_region_stats(region, newspaper, news_count)

        logger.info(f"âœ“ {inserted}ê°œ ê¸°ì‚¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        
        # 30ì¼ ì´ì „ ê¸°ì‚¬ ìë™ ì‚­ì œ
        self.db_manager.delete_old_articles(days=30)

        # í†µê³„ ì¶œë ¥
        self.db_manager.print_stats()

    def save_as_text_files(self):
        """ì›ë³¸ ë‰´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.save_text_files:
            logger.warning("í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not self.all_articles:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"\n{'=' * 70}")
        logger.info("ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘...")
        logger.info(f"{'=' * 70}")

        # ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
        saved_count = self.text_saver.save_articles(self.all_articles)

        # ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
        self.text_saver.create_index_file(self.all_articles)

        logger.info(f"âœ“ {saved_count}ê°œ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")

    def save_all(self, csv_filename: str = '../../data/regional_news.csv'):
        """
        ëª¨ë“  í¬ë§·ìœ¼ë¡œ ì €ì¥ (CSV + ë°ì´í„°ë² ì´ìŠ¤ + í…ìŠ¤íŠ¸ íŒŒì¼)

        Args:
            csv_filename: CSV íŒŒì¼ ê²½ë¡œ
        """
        logger.info(f"\n{'=' * 70}")
        logger.info("ğŸ’¾ ë°ì´í„° ì €ì¥ ì‹œì‘")
        logger.info(f"{'=' * 70}\n")

        # 1. CSV ì €ì¥
        self.save_to_csv(csv_filename)

        # 2. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if self.use_database:
            self.save_to_database()

        # 3. í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
        if self.save_text_files:
            self.save_as_text_files()

        logger.info(f"\n{'=' * 70}")
        logger.info("âœ… ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
        logger.info(f"{'=' * 70}\n")

    def print_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        df = self.to_dataframe()

        if df.empty:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"\n{'=' * 70}")
        logger.info("ğŸ“Š ìˆ˜ì§‘ í†µê³„")
        logger.info(f"{'=' * 70}")

        # ì§€ì—­ë³„ í†µê³„
        logger.info("\nğŸ“ ì§€ì—­ë³„ ê¸°ì‚¬ ìˆ˜:")
        region_stats = df.groupby('region').size().sort_values(ascending=False)
        for region, count in region_stats.items():
            logger.info(f"  {region}: {count}ê°œ")

        # ì‹ ë¬¸ì‚¬ë³„ í†µê³„
        logger.info("\nğŸ“° ì‹ ë¬¸ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        newspaper_stats = df.groupby('source').size().sort_values(ascending=False)
        for source, count in newspaper_stats.items():
            logger.info(f"  {source}: {count}ê°œ")

        logger.info(f"\n{'=' * 70}\n")

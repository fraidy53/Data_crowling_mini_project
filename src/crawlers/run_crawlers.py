"""
ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì „êµ­ ì§€ì—­ë³„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹¤í–‰
"""

import sys
import argparse
from crawler_manager import CrawlerManager


def main():
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(
        description='ì„œìš¸, ê²½ê¸°ë„, ê°•ì›ë„ ì§€ì—­ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # ëª¨ë“  ì§€ì—­ í¬ë¡¤ë§ (ê° ì‹ ë¬¸ 50ê°œ ê¸°ì‚¬)
  python run_crawlers.py --mode all --articles 50

  # ì„œìš¸ë§Œ í¬ë¡¤ë§
  python run_crawlers.py --mode region --region ì„œìš¸ --articles 30

  # ê²½ê¸°ë„ë§Œ í¬ë¡¤ë§
  python run_crawlers.py --mode region --region ê²½ê¸°ë„ --articles 30
        '''
    )

    parser.add_argument(
        '--mode',
        choices=['all', 'region'],
        default='all',
        help='í¬ë¡¤ë§ ëª¨ë“œ (all: ì „ì²´ ì§€ì—­, region: íŠ¹ì • ì§€ì—­)'
    )
    parser.add_argument(
        '--region',
        type=str,
        choices=['ì„œìš¸', 'ê²½ê¸°ë„', 'ê°•ì›ë„'],
        default='ì„œìš¸',
        help='ì§€ì—­ ì„ íƒ (ì„œìš¸, ê²½ê¸°ë„, ê°•ì›ë„)'
    )
    parser.add_argument(
        '--articles',
        type=int,
        default=50,
        help='ì‹ ë¬¸ì‚¬ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 50)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='../../data/regional_news.csv',
        help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ../../data/regional_news.csv)'
    )
    parser.add_argument(
        '--save-db',
        action='store_true',
        default=True,
        help='ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ê¸°ë³¸ê°’: True)'
    )
    parser.add_argument(
        '--save-text',
        action='store_true',
        default=True,
        help='í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ (ê¸°ë³¸ê°’: True)'
    )

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("ğŸ•·ï¸  ì§€ì—­ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    print("=" * 70)
    print(f"ëª¨ë“œ: {args.mode}")
    if args.mode == 'region':
        print(f"ëŒ€ìƒ ì§€ì—­: {args.region}")
    print(f"ì‹ ë¬¸ì‚¬ë‹¹ ê¸°ì‚¬ ìˆ˜: {args.articles}ê°œ")
    print(f"CSV ì¶œë ¥: {args.output}")
    print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {'ì˜ˆ' if args.save_db else 'ì•„ë‹ˆì˜¤'}")
    print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥: {'ì˜ˆ' if args.save_text else 'ì•„ë‹ˆì˜¤'}")
    print("=" * 70 + "\n")

    # í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ìƒì„±
    manager = CrawlerManager(
        use_database=args.save_db,
        save_text_files=args.save_text
    )
    manager.register_all_crawlers()

    # í¬ë¡¤ë§ ì‹¤í–‰
    if args.mode == 'all':
        manager.run_all_crawlers(max_articles=args.articles)
    else:
        manager.run_by_region(args.region, max_articles=args.articles)

    # ê²°ê³¼ ì €ì¥ (ëª¨ë“  í¬ë§·)
    manager.save_all(csv_filename=args.output)

    print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == '__main__':
    main()

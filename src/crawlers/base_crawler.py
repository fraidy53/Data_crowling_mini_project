"""
Base Crawler í´ë˜ìŠ¤
ëª¨ë“  ì§€ì—­ ì‹ ë¬¸ í¬ë¡¤ëŸ¬ì˜ ë¶€ëª¨ í´ë˜ìŠ¤
"""

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import logging
from abc import ABC, abstractmethod
import time
from typing import List, Dict, Optional
import pandas as pd

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(message)s'
)


class BaseCrawler(ABC):
    """
    ëª¨ë“  ì§€ì—­ ì‹ ë¬¸ í¬ë¡¤ëŸ¬ì˜ ë¶€ëª¨ í´ë˜ìŠ¤
    ê° ì‹ ë¬¸ì‚¬ë³„ë¡œ ìƒì†í•˜ì—¬ ì‚¬ìš©

    ì‚¬ìš© ë°©ì‹:
        1. newspaper_name: ì‹ ë¬¸ì‚¬ëª…
        2. region: ì§€ì—­ëª…
        3. base_url: ë©”ì¸ URL
        4. config: CSS ì„ íƒì ë“± ì„¤ì •
    """

    def __init__(self,
                 newspaper_name: str,
                 region: str,
                 base_url: str,
                 config: Dict):
        """
        Args:
            newspaper_name: ì‹ ë¬¸ì‚¬ëª… (ì˜ˆ: ì„œìš¸ì‹ ë¬¸, ê²½ê¸°ì¼ë³´)
            region: ì§€ì—­ëª… (ì˜ˆ: ì„œìš¸, ê²½ê¸°ë„)
            base_url: ë©”ì¸ URL
            config: CSS ì„ íƒì ë”•ì…”ë„ˆë¦¬
        """
        self.newspaper_name = newspaper_name
        self.region = region
        self.base_url = base_url
        self.config = config
        self.logger = logging.getLogger(newspaper_name)

        # User-Agent (ë´‡ìœ¼ë¡œ ì°¨ë‹¨ë‹¹í•˜ì§€ ì•Šë„ë¡)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        self.articles = []
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    @abstractmethod
    def get_article_urls(self) -> List[str]:
        """
        ì‹ ë¬¸ì‚¬ì˜ ê²½ì œì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ëª©ë¡ ì¶”ì¶œ
        ê° ì‹ ë¬¸ì‚¬ì˜ URL êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        """
        pass

    @abstractmethod
    def parse_article(self, url: str) -> Optional[Dict]:
        """
        ê°œë³„ ê¸°ì‚¬ íŒŒì‹±
        ì œëª©, ë³¸ë¬¸, ë‚ ì§œ, ì¶œì²˜ ë“± ì¶”ì¶œ
        """
        pass

    def fetch_page(self, url: str, use_selenium: bool = False, retries: int = 3) -> Optional[BeautifulSoup]:
        """
        HTML í˜ì´ì§€ ìš”ì²­ ë° íŒŒì‹± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

        Args:
            url: ìš”ì²­ URL
            use_selenium: JavaScript ë Œë”ë§ í•„ìš” ì—¬ë¶€
            retries: ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            BeautifulSoup ê°ì²´ ë˜ëŠ” None
        """
        for attempt in range(retries):
            try:
                if use_selenium:
                    return self._fetch_with_selenium(url)
                response = self.session.get(url, timeout=15)

                # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ì„¤ì •
                if response.encoding and response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                    response.encoding = 'utf-8'
                elif not response.encoding:
                    response.encoding = response.apparent_encoding or 'utf-8'

                if response.status_code == 200:
                    self.logger.debug(f"âœ“ í˜ì´ì§€ ë¡œë“œ: {url[:60]}...")
                    return BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')

                self.logger.warning(f"âœ— ìƒíƒœ ì½”ë“œ {response.status_code}: {url}")
                return None

            except requests.Timeout:
                if attempt < retries - 1:
                    self.logger.warning(f"â± íƒ€ì„ì•„ì›ƒ (ì¬ì‹œë„ {attempt + 1}/{retries}): {url[:60]}...")
                    time.sleep(1)
                else:
                    self.logger.error(f"âœ— íƒ€ì„ì•„ì›ƒ (ìµœì¢… ì‹¤íŒ¨): {url}")
                    return None

            except (requests.ConnectionError, requests.exceptions.ChunkedEncodingError) as e:
                if attempt < retries - 1:
                    self.logger.warning(f"ğŸ”„ ì—°ê²° ì˜¤ë¥˜ (ì¬ì‹œë„ {attempt + 1}/{retries}): {url[:60]}...")
                    time.sleep(2)
                else:
                    self.logger.error(f"âœ— ì—°ê²° ì‹¤íŒ¨ (ìµœì¢…): {e}")
                    return None

            except Exception as e:
                if attempt < retries - 1:
                    self.logger.warning(f"âš  ì˜¤ë¥˜ (ì¬ì‹œë„ {attempt + 1}/{retries}): {e}")
                    time.sleep(1)
                else:
                    self.logger.error(f"âœ— í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return None

        return None

    def _fetch_with_selenium(self, url: str) -> Optional[BeautifulSoup]:
        """Seleniumì„ ì‚¬ìš©í•œ JavaScript ë Œë”ë§ í˜ì´ì§€ ë¡œë“œ"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')

            driver = webdriver.Chrome(options=options)
            driver.get(url)

            # ì»¨í…ì¸  ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'body'))
            )

            html = driver.page_source
            driver.quit()

            return BeautifulSoup(html, 'html.parser')
        except Exception as e:
            self.logger.error(f"âœ— Selenium ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def extract_text(self, element, selector: str, default: str = 'N/A') -> str:
        """
        CSS ì„ íƒìë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜)

        Args:
            element: BeautifulSoup ìš”ì†Œ
            selector: CSS ì„ íƒì
            default: ì°¾ì§€ ëª»í–ˆì„ ë•Œ ê¸°ë³¸ê°’

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        if not element:
            return default

        try:
            elem = element.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
            return default
        except Exception:
            return default

    def crawl(self, max_articles: int = 50) -> List[Dict]:
        """
        ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤

        Args:
            max_articles: ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜

        Returns:
            ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"[{self.newspaper_name}({self.region})] í¬ë¡¤ë§ ì‹œì‘")
        self.logger.info(f"{'=' * 60}")

        try:
            # 1ë‹¨ê³„: ê¸°ì‚¬ URL ìˆ˜ì§‘
            self.logger.info("1ë‹¨ê³„: ê¸°ì‚¬ URL ìˆ˜ì§‘ ì¤‘...")
            article_urls = self.get_article_urls()

            if not article_urls:
                self.logger.warning("ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                return self.articles

            article_urls = article_urls[:max_articles]
            self.logger.info(f"âœ“ {len(article_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ")

            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ íŒŒì‹±
            self.logger.info(f"2ë‹¨ê³„: {len(article_urls)}ê°œ ê¸°ì‚¬ íŒŒì‹± ì¤‘...")

            for idx, url in enumerate(article_urls, 1):
                self.logger.info(f"  [{idx}/{len(article_urls)}] íŒŒì‹±...")

                article = self.parse_article(url)
                if article:
                    article['newspaper'] = self.newspaper_name
                    article['region'] = self.region
                    self.articles.append(article)

                # ì„œë²„ ë¶€í•˜ ë°©ì§€ (ìš”ì²­ ê°„ 1ì´ˆ ëŒ€ê¸°)
                time.sleep(1)

            self.logger.info(f"âœ“ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            self.logger.info(f"{'=' * 60}\n")

            return self.articles

        except Exception as e:
            self.logger.error(f"âœ— í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return self.articles

    def to_dataframe(self) -> pd.DataFrame:
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        if not self.articles:
            return pd.DataFrame()
        return pd.DataFrame(self.articles)

    def save_to_csv(self, filename: str):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        df = self.to_dataframe()
        if df.empty:
            self.logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df.to_csv(filename, index=False, encoding='utf-8-sig')
        self.logger.info(f"âœ“ íŒŒì¼ ì €ì¥: {filename}")

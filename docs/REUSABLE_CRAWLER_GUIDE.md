# ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ëª¨ë“ˆ ì„¤ê³„

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
src/crawlers/
â”œâ”€â”€ utils/                      # ì¬ì‚¬ìš© ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ content_parser.py       # ë³¸ë¬¸ íŒŒì‹±
â”‚   â”œâ”€â”€ date_parser.py          # ë‚ ì§œ/ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
â”‚   â””â”€â”€ text_cleaner.py         # í…ìŠ¤íŠ¸ ì •ì œ
â”‚
â”œâ”€â”€ newspaper_factory.py        # ì‹ ë¬¸ì‚¬ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬
â”œâ”€â”€ base_crawler.py             # ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
â”‚
â””â”€â”€ examples/                   # ì‚¬ìš© ì˜ˆì‹œ
    â””â”€â”€ how_to_use_factory.py
```

---

## ğŸ¯ ì£¼ìš” í´ë˜ìŠ¤

### 1. ContentParser (ë³¸ë¬¸ íŒŒì‹±)
```python
from utils import ContentParser

# ë°©ë²• 1: CSS ì„ íƒìë¡œ ì¶”ì¶œ
content = ContentParser.extract_from_selector(
    soup, 
    ['div.article-body', 'article.content']
)

# ë°©ë²• 2: p íƒœê·¸ì—ì„œ ì¶”ì¶œ
content = ContentParser.extract_from_paragraphs(
    soup,
    container_selector='div.article'
)

# ë°©ë²• 3: í…ìŠ¤íŠ¸ ë¼ì¸ìœ¼ë¡œ ì¶”ì¶œ (br íƒœê·¸ êµ¬ë¶„)
content = ContentParser.extract_from_textlines(
    soup,
    container_selector='div.content'
)
```

### 2. DateParser (ë‚ ì§œ/ì‘ì„±ì ì¶”ì¶œ)
```python
from utils import DateParser

text = "ìŠ¹ì¸ 2026-02-23 15:30 | í™ê¸¸ë™ ê¸°ì"

date = DateParser.extract_date(text)      # "2026-02-23"
writer = DateParser.extract_writer(text)  # "í™ê¸¸ë™"
```

### 3. TextCleaner (í…ìŠ¤íŠ¸ ì •ì œ)
```python
from utils import TextCleaner

dirty = "ë‰´ìŠ¤   ë‚´ìš©  https://example.com  hong@email.com"
clean = TextCleaner.clean_article_text(dirty)
# "ë‰´ìŠ¤ ë‚´ìš©"

sentences = TextCleaner.extract_sentences("ì²« ë¬¸ì¥. ë‘ ë²ˆì§¸! ì„¸ ë²ˆì§¸?")
# ["ì²« ë¬¸ì¥", "ë‘ ë²ˆì§¸", "ì„¸ ë²ˆì§¸"]
```

---

## ğŸ­ NewspaperFactory (ì‹ ë¬¸ì‚¬ ì¶”ê°€)

### ë°©ë²• 1: ì‚¬ì „ ì •ì˜ëœ ì‹ ë¬¸ì‚¬ ì‚¬ìš©
```python
from newspaper_factory import NewspaperFactory

# ì„œìš¸ì‹ ë¬¸ í¬ë¡¤ëŸ¬ ìë™ ìƒì„±
crawler = NewspaperFactory.create('ì„œìš¸ì‹ ë¬¸')
articles = crawler.crawl(max_articles=10)

# ì‚¬ìš© ê°€ëŠ¥í•œ ì‹ ë¬¸ì‚¬ ëª©ë¡
print(NewspaperFactory.list_available())
# ['ì„œìš¸ì‹ ë¬¸', 'ê²½ê¸°ì¼ë³´', 'ê°•ì›ë„ë¯¼ì¼ë³´']
```

### ë°©ë²• 2: ìƒˆ ì‹ ë¬¸ì‚¬ ì¶”ê°€ (ì„¤ì •ë§Œìœ¼ë¡œ!)
```python
from newspaper_factory import NewspaperFactory, NewspaperConfig

# 1. ì„¤ì • ì‘ì„± (ì½”ë“œ 5ì¤„)
config = NewspaperConfig(
    newspaper_name='ë¶€ì‚°ì¼ë³´',
    region='ë¶€ì‚°',
    base_url='https://www.busan.com',
    list_url='https://www.busan.com/news/economy',
    article_link_selector='div.news-list a',
    content_selectors=['div.article-body', 'div.content'],
    parsing_method='paragraphs'
)

# 2. í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰ (ì½”ë“œ 2ì¤„)
crawler = NewspaperFactory.create_custom(config)
articles = crawler.crawl(max_articles=10)
```

---

## ğŸ“ íŒŒì‹± ë°©ë²• ì„ íƒ ê°€ì´ë“œ

| íŒŒì‹± ë°©ë²• | ì–¸ì œ ì‚¬ìš©? | ì˜ˆì‹œ |
|---------|---------|------|
| `selector` | ëª…í™•í•œ ë³¸ë¬¸ divê°€ ìˆì„ ë•Œ | `<div class="article-body">ë³¸ë¬¸</div>` |
| `paragraphs` | p íƒœê·¸ë¡œ êµ¬ì„±ë  ë•Œ | `<p>ë¬¸ë‹¨1</p><p>ë¬¸ë‹¨2</p>` |
| `textlines` | br íƒœê·¸ë¡œ êµ¬ë¶„ë  ë•Œ | `í…ìŠ¤íŠ¸1<br>í…ìŠ¤íŠ¸2<br>` |

---

## ğŸš€ ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: 10ê°œ ì§€ì—­ ì‹ ë¬¸ í•œë²ˆì— ì¶”ê°€
```python
regions = [
    ('ì¶©ì²­ì¼ë³´', 'ì¶©ì²­ë„', 'https://www.ccdailynews.com'),
    ('ì „ë¼ì¼ë³´', 'ì „ë¼ë„', 'https://www.jeollailbo.com'),
    ('ì œì£¼ì¼ë³´', 'ì œì£¼ë„', 'https://www.jejuilbo.net'),
    # ... 7ê°œ ë”
]

for name, region, base_url in regions:
    config = NewspaperConfig(
        newspaper_name=name,
        region=region,
        base_url=base_url,
        list_url=f'{base_url}/news/economy',
        article_link_selector='a.article',
        content_selectors=['div.article-body'],
        parsing_method='paragraphs'
    )
    
    crawler = NewspaperFactory.create_custom(config)
    articles = crawler.crawl(max_articles=10)
    print(f"{name}: {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
```

### ì˜ˆì‹œ 2: í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ì— í†µí•©
```python
from crawler_manager import CrawlerManager
from newspaper_factory import NewspaperFactory

manager = CrawlerManager()

# ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤
manager.register_crawler(NewspaperFactory.create('ì„œìš¸ì‹ ë¬¸'))
manager.register_crawler(NewspaperFactory.create('ê²½ê¸°ì¼ë³´'))

# ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ ì‰½ê²Œ ì¶”ê°€
busan_crawler = NewspaperFactory.create_custom(busan_config)
manager.register_crawler(busan_crawler)

manager.run_all_crawlers()
```

---

## âœ¨ ì¥ì 

1. **ì½”ë“œ ì¬ì‚¬ìš©**: ê³µí†µ ë¡œì§ì„ ìœ í‹¸ë¦¬í‹°ë¡œ ë¶„ë¦¬
2. **ì‰¬ìš´ í™•ì¥**: ì„¤ì •ë§Œìœ¼ë¡œ ìƒˆ ì‹ ë¬¸ì‚¬ ì¶”ê°€ (5ë¶„ ì†Œìš”)
3. **ì¼ê´€ëœ í’ˆì§ˆ**: ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ë™ì¼í•œ ì •ì œ ë¡œì§ ì‚¬ìš©
4. **ìœ ì§€ë³´ìˆ˜ ìš©ì´**: í•œ ê³³ ìˆ˜ì •ìœ¼ë¡œ ëª¨ë“  í¬ë¡¤ëŸ¬ ê°œì„ 

---

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ContentParser í•„í„° ì¶”ê°€
```python
# utils/content_parser.py

ContentParser.NOISE_KEYWORDS.extend([
    'ì¶”ê°€í‚¤ì›Œë“œ1',
    'ì¶”ê°€í‚¤ì›Œë“œ2'
])
```

### DateParser íŒ¨í„´ ì¶”ê°€
```python
# utils/date_parser.py

DateParser.DATE_PATTERNS.append(
    r'ë°œí–‰\s*(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)'
)
```

---

## ğŸ“Š ì„±ëŠ¥

- **ê¸°ì¡´ ë°©ì‹**: ì‹ ë¬¸ì‚¬ë‹¹ 100ì¤„ ì½”ë“œ, ì¤‘ë³µ ë§ìŒ
- **ê°œì„  ë°©ì‹**: ì‹ ë¬¸ì‚¬ë‹¹ 5ì¤„ ì„¤ì •, ì¤‘ë³µ ì œê±°

**ì½”ë“œ ê°ì†Œìœ¨: 95% â†“**

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ì¬ì‚¬ìš© ëª¨ë“ˆ ì™„ì„±
2. ğŸ”„ ê¸°ì¡´ í¬ë¡¤ëŸ¬ë¥¼ Factory ë°©ì‹ìœ¼ë¡œ ì „í™˜
3. ğŸ“ˆ ìƒˆ ì§€ì—­ ì‹ ë¬¸ 10ê°œ ì¶”ê°€
4. ğŸ¤– AI ê¸°ë°˜ ìë™ ì„ íƒì íƒì§€ (ê³ ê¸‰)

---

## ğŸ“ ë¬¸ì˜

ìƒˆë¡œìš´ ì‹ ë¬¸ì‚¬ ì¶”ê°€ë‚˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ í•„ìš”í•˜ë©´ `newspaper_factory.py`ì˜ PRESETSì— ì¶”ê°€í•˜ê±°ë‚˜, ì»¤ìŠ¤í…€ ì„¤ì •ì„ ì‚¬ìš©í•˜ì„¸ìš”!
